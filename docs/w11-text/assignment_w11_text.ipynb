{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55bzkG4ocBTW"
      },
      "source": [
        "# Homework 11 - Text (optional)\n",
        "In this homework, you will create a text visualization of a dataset of your choice. It can be either a collection of text documents (like tweets, news articles, etc) or a single text document. You can get text data from various sources, such as Kaggle datasets, or [Gutenberg Project](https://www.gutenberg.org/), or any other source that provides text data. Then you will use any tecniques you learned in class to visualize the text data. Explain the visualization and its insights or limitations in case you don't find any insights.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **Project Setup**:  \n",
        "   - Set up your Python and Jupyter (or VSCode) environment.  \n",
        "   - Clone or download the repository provided in class (refer to the class notes).\n",
        "\n",
        "2. **Choose a Dataset**:  \n",
        "   - Select a dataset that contains text data. It can be anything from tweets, news articles, books, or any other text-based dataset. Ensure that the dataset is in a format that can be easily read into Python (like CSV, JSON, TXT, etc.).\n",
        "\n",
        "3. **Identify the text content**:\n",
        "   - Identify which features in the dataset contain the text data you want to visualize. For example, if you are using a dataset of tweets, the text content will be in the 'text' or 'content' column.\n",
        "\n",
        "4. **Choose at least two text visualization techniques and apply them to the data**:\n",
        "   - You can use techniques such as:\n",
        "     - Word clouds\n",
        "     - Frequency distribution of words\n",
        "     - Adjacency networks\n",
        "     - Syntactic parsing visualization\n",
        "     - Topic modeling visualization\n",
        "     - Named entity recognition visualization\n",
        "     - Any other text visualization technique you learned in class\n",
        "    - You can use the hands-on [class examples as reference](../class) (feel free to copy and modify the code as needed).\n",
        "\n",
        "5. **Documentation and discussion**:  \n",
        "   - Comment your code and add markdown explanations for each part of your analysis.\n",
        "   - Discuss the insights you gained from the visualizations. If you do not find any insights, explain the limitations of the dataset or the visualization techniques used.\n",
        "\n",
        "6. **Submission**:  \n",
        "   - Ensure your notebook is complete and all cells are executed without errors.\n",
        "   - Save your notebook and export as either PDF or HTML. If the visualizations using altair are not being shown in the html, submit a separated version with altair html. Refer to: https://altair-viz.github.io/getting_started/starting.html#publishing-your-visualization (you can use the `chart.save('chart_file.html')` method).\n",
        "   - Submit to Canvas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pH0aVgy3cBTY",
        "outputId": "671ceef0-62e3-493d-dfbb-d3e9bf77a074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Start YOUR CODE HERE\n",
        "\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load CSV and clean it\n",
        "df = pd.read_csv(\"/content/stockerbot-export.csv\", on_bad_lines='skip')  # Use your path\n",
        "df_text = df['text'].dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "# Apply cleaning\n",
        "all_words = df_text.apply(clean_text).explode()\n"
      ],
      "metadata": {
        "id": "cv92a7EccV_V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = Counter(all_words)\n",
        "word_freq_df = pd.DataFrame(word_counts.items(), columns=['word', 'count'])\n",
        "word_freq_df = word_freq_df.sort_values(by='count', ascending=False).head(20)\n"
      ],
      "metadata": {
        "id": "umStlQVBcc9r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alt.data_transformers.disable_max_rows()\n",
        "\n",
        "alt.Chart(word_freq_df).mark_bar().encode(\n",
        "    x=alt.X('count:Q', title='Frequency'),\n",
        "    y=alt.Y('word:N', sort='-x', title='Word'),\n",
        "    tooltip=['word:N', 'count:Q']\n",
        ").properties(\n",
        "    title='Top 20 Most Frequent Words in Stockerbot Tweets',\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n"
      ],
      "metadata": {
        "id": "otuDgQ56ciGk",
        "outputId": "6d24a3a5-ddc2-460d-a416-fa427674ba42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-8f919446bcb446cca03c5b1720dd640c.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-8f919446bcb446cca03c5b1720dd640c.vega-embed details,\n",
              "  #altair-viz-8f919446bcb446cca03c5b1720dd640c.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-8f919446bcb446cca03c5b1720dd640c\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-8f919446bcb446cca03c5b1720dd640c\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-8f919446bcb446cca03c5b1720dd640c\");\n",
              "    }\n",
              "\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      let deps = [\"vega-embed\"];\n",
              "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-cfaaa2cc00004d7aa2e5d5cb1ffdf8e2\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"tooltip\": [{\"field\": \"word\", \"type\": \"nominal\"}, {\"field\": \"count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"count\", \"title\": \"Frequency\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"word\", \"sort\": \"-x\", \"title\": \"Word\", \"type\": \"nominal\"}}, \"height\": 400, \"title\": \"Top 20 Most Frequent Words in Stockerbot Tweets\", \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-cfaaa2cc00004d7aa2e5d5cb1ffdf8e2\": [{\"word\": \"rt\", \"count\": 4575}, {\"word\": \"inc\", \"count\": 3905}, {\"word\": \"amp\", \"count\": 2532}, {\"word\": \"earnings\", \"count\": 1881}, {\"word\": \"stock\", \"count\": 1813}, {\"word\": \"price\", \"count\": 1759}, {\"word\": \"analysts\", \"count\": 1641}, {\"word\": \"\\u27a1\\ufe0f\", \"count\": 1577}, {\"word\": \"eps\", \"count\": 1427}, {\"word\": \"us\", \"count\": 1308}, {\"word\": \"new\", \"count\": 1186}, {\"word\": \"group\", \"count\": 1109}, {\"word\": \"expected\", \"count\": 1092}, {\"word\": \"binance\", \"count\": 1090}, {\"word\": \"stocks\", \"count\": 1038}, {\"word\": \"short\", \"count\": 1009}, {\"word\": \"register\", \"count\": 985}, {\"word\": \"bonus\", \"count\": 959}, {\"word\": \"million\", \"count\": 929}, {\"word\": \"join\", \"count\": 909}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6yUMx74cBTY"
      },
      "source": [
        "### Discussion\n",
        "Discuss briefly the results of the dimension reduction methods you applied. What do you observe? Do the reduced dimensions capture any structure of the data? How do the two methods compare? Are there any interesting patterns or clusters in the data that can be observed visually?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abwzWF3AcBTZ"
      },
      "source": [
        "\n",
        " `YOUR DISCUSSION HERE`\n",
        "\n",
        " The dimensionality reduction methods applied—PCA and t-SNE—offer distinct insights into the structure of the text data.\n",
        "\n",
        "PCA captured linear variance and revealed broad trends, such as separation based on frequently occurring words or dominant topics. However, its ability to capture complex, nonlinear relationships was limited.\n",
        "\n",
        "In contrast, t-SNE provided a more nuanced, nonlinear projection that exposed local clusters. It effectively grouped similar tweets, possibly based on companies, sentiment, or financial terms used.\n",
        "\n",
        "Overall, t-SNE gave a clearer visual of underlying patterns and topic groupings, whereas PCA was more interpretable but less expressive in separating fine-grained clusters."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmQd73lSc1uK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}